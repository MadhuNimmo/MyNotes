* Why do we need Docker? 
  - Compatibility and Dependency between different services and the OS
  - Long setup time
  - Different dev/test/prod environments
* What can it do?
  - Allows us to change or modify the componenets without affecting the other componenets and even modify the 
  underlying oS as required. With Docker we can run each componenet in a seprate container with its own dependencies
  and its own libraries, all on the same VM and the OS, but within separate environments or containers.
  Just build the Docker configuration once and all developers and use the same irrespective of their underlying system.
  - Containerize apps
  - Run each service with its own dependencies in separate containers
* What are containers? 
  - Completely isolated environments. As in they have their own processes or services, their own network interfaces, 
  their own mounts, except they all share the same OS kernel.
  - Containers has existed in the past as well.
  - Docker uses LXC containers
  - Setting up this containers is hard as they are very low level , and that is where docker 
  comes useful. Docker offers a high level tool with several functionalities making it easy for us.
* Basic Concepts of Operating System :
  - every os consists of two things - an os kernel and a set of software
  - kernel is responsible for interacting with the underlying hardware
  - software above makes the OS different , the software may consist of a diff user interface drivers,
  compilers, file managers, developer tools etc.
  - Ubuntu, Fedore,Susi,CentOS - a common linux kernel shared across all OSS and some custom software that differentiate OSs from each other.
  - Suppose we have a system with an ubuntu OS with docker installed on it. Docker can run any flavor of OS on it as long as they are based on the same kernel.
    Dockeer only has the necessary software which make the OSs different and docker utilizes the underlying kernel of the docker host which works with all OSS above.
    So, you cannot run a windows based container on a docker host with linux on it. For that we would need a Docker on a windows server.
    But it is made possible in a way that, windows based container is run on a windows VM under the hoods. 
* Containers VS Virual Machines :
  - For Docker, we have the underlying infrastructure, the OS, and then Docker installed on the OS.Docker then manages the containers that 
    run with dependencies and libraries alone. In case of VMs, we have the hypervisors like ESx on the hardware and then the VMs on them, each VM has its own OS upon it, then the dependencies and libraries 
    and finally the applications.
  - The overheard causes higher utilization of resources as there are multiple virtual operating systems and kernels running. The VMs also consumed 
    higher disk space as each VM is heavy and usually in GB, whereas Docker is lighweight and in MBs. As a results, Docker containers boot up faster.
  - Dockers have less isolation as the kernel is shared in comprison with the VMs. 
* Containers & Virual Machines :
  - You will often see containers provisioned on virtual docker hosts. That way we can utilize the advantages 
  of both technologies, we can use the benefits of virtualization to easily provision or decommission Docker hosts as required
  at the same time make use of the benefits of Docker to easily provision applications and quickly scale them as required. But, now 1 Vm
  can support many applications.
* How is it done?
  - public Docker repo - Docker Hub / Docker Store
  - Ex: we can find images of most common OSs, DBs and other services and tools. Once you identify the images we need and you have Docker in your host,
    bringing up an app is as easy as a Docker run cmd. Docker run <<image>>
  - If we need to run multiple instances of the web service , simply ass as many instances as you need and configure
    a load balancer of some kind in the front. In case one instance fails, simply destroy it and launch anyone.
    There are other solutions available.
* Container vs Image
  - An image is a package or template that is used to create one or more containers
  - Containers are running instances of images that are isolated and have their own envs and set of processes.
* Docker in DevOps
  - With Docker developers and Ops team work hand in hand to transform guide in a Docker file with both of their requirements.
    This Docker file is then used to craete an image, whioch now can run on any host in the same way everywhere.  
* Docker Editions
  - Community - free Docker products
  - Enterprise - supported container platform with added support
* Docker commands
  - run followed by img-name -> start a container if an instance of the application already exists else it will pull the image down
  - ps -> list all running containers and some basic info about them (cont id, cont name ...)
  - ps -a -> list all containers running/not
  - stop followed by container-name -> to stop a container from running
  - rm followed by container-name -> to remove a container
  - images -> list available images and their sizes
  - rmi followed by img-name -> to remove an image
  - pull followed by img-name -> to only download the image and store, not run a container
  - exec followed by container and arguments-> to execute a command
  - run followed by web-service -> to run a service on a host, attach the output to the console/std put of container.
  - run -d followed by web-service -> will run the container in the background and you will be back to the prompt immediately
  - attach followed by container id -> to attach back to the running container later
* More on Run
  - run XYZ:version -> version is also called tag, important when we need a specific version, no tag - latest version, docker hub will provide
    all version details
  - run -i XYZ -> for interactive running, when we need to provide input from console
  - run -it XYX -> allows to attach a pseudo terminal and show prompts to receive inputs
  - run -p 80:5000 XYZ-> to run an app on a server, to access it from a web browser, either use the ip of the docker container(only accessible by 
    the docker host) or use the ip of the docker host , but to make that work you need to map the post inside the docker container to 
    a free port on the docker host. here we map local port 80 to docker port 5000. You can run mutiple instances on diff ports as well.
  - run -v ABC/DEF:WX:YZ-> stores all data/files/databases in container file system.To keep them even after you delete the container you need to 
    map the container specific files to a directory outside the container. In this case, host file ABC/EFG is mapped to container file WX/YZ.
  - inspect cont-name/cont-id -> add info on containers
  - logs cont-name/cont-id -> to see the execution logs
* Docker Environment Variables
  - run -e Env-Var=Val XYZ-> to run an app with env vars set to certain values
  - inspect XYZ -> to find the env vars under config
* Docker Images
  - how to create my own img? 
      - write down teh instructions
      - create a docker file to write down the instructions for setting up an application in it
      - once done build your image using the docker build command and specify the docker file as input and a tage name foe the image
      - to make avaiable publicly use docker push command followed by accntname/imagename
      - Docker file is a text file in inst and arg format 
* CMD and ENTRYPOINTS
  - CMD -> command and parameters can be provided as a list/json
  - ENTRYPOINT -> has the command embedded, only need to pass the argument
  - use both entry point and cmd in json format to have default option
* Netwrking
  - Bridge - default network, otherwise specify network using network=node/host
    get internal ip address, they can access eachother using the ips, to access from outside, map the ports to docker host/
    associate the container with the host network, remove any isolation between the host and container.
  - None - not attached to any network. they run in an isolated network
  - Host - 
  - use network create/network inspect to create or adjust the settings
  - containers can reach each other using their names
* Storage
  - var/lib/docker - stores all data by default
  - only the container layer is read-write
  - image layer is shared between containers and is read only
  - all data for container gets deleted when its removed
  - you can create a volume to store data beyond the life of the container
  - you can also mount previously avaiable data for your use in your app
* Compose
  - link is used to rlate between two containers
  - the compose.yml mentions the images,links and ports with already built images
* Registry
  - images org-user/img-name default storage is docker registry
